{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `compute_distance_sums` Function\n",
    "\n",
    "The `compute_distance_sums` function calculates the cumulative distance between a given test image embedding and a set of stored memory embeddings. The computed distances are aggregated based on class labels, providing a per-class similarity score.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **selected_test_image_embedding** (`numpy array`): The embedding vector of the test image.\n",
    "- **memory_embeddings** (`list of numpy arrays`): A collection of embeddings stored in memory.\n",
    "- **memory_labels** (`list of int`): Class labels corresponding to `memory_embeddings`.\n",
    "- **predicted_softmax_label** (`numpy array`, optional): A probability distribution over classes, from the finetuned model.\n",
    "- **static_distance** (`float`, default=`1`): A scaling factor applied to `predicted_softmax_label`.\n",
    "- **distance_metric** (`str`, default=`'cosine'`): Specifies the distance metric to use. Options:\n",
    "  - `'cosine'`: Uses cosine similarity.\n",
    "  - `'euclidean'`: Uses a modified Euclidean distance (`1 / (1 + d)`).\n",
    "- **num_classes** (`int`, default=`10`): Total number of possible classes.\n",
    "\n",
    "## Returns\n",
    "\n",
    "- **distance_sums** (`numpy array`): An array of length `num_classes`, where each index represents the cumulative similarity score for that class.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### 1. One-Hot Encoding of Labels\n",
    "- A one-hot encoded matrix of shape `(len(memory_labels), num_classes)` is created to represent the class labels.\n",
    "- If any label is out of range, an error is raised.\n",
    "\n",
    "### 2. Distance Computation\n",
    "- Each memory embedding is compared to the `selected_test_image_embedding` using the specified `distance_metric`.\n",
    "  - **Cosine similarity** is used if `distance_metric='cosine'`.\n",
    "  - A **normalized Euclidean distance** (`1 / (1 + d)`) is used if `distance_metric='euclidean'`.\n",
    "- The computed distance is multiplied by the corresponding one-hot encoded label and added to `distance_sums`.\n",
    "\n",
    "### 3. Adjustment Using Softmax Prediction\n",
    "- If `predicted_softmax_label` is provided, it is scaled by `static_distance` and added to `distance_sums`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_sums(selected_test_image_embedding, memory_embeddings, memory_labels, predicted_softmax_label= None, static_distance = 1, distance_metric='cosine', num_classes=10):\n",
    "\n",
    "    distance_sums = np.zeros(num_classes)\n",
    "\n",
    "    # Initialize the one-hot vectors based on memory_labels\n",
    "    memory_labels_onehot = np.zeros((len(memory_labels), num_classes))\n",
    "\n",
    "    # Create the one-hot encoding for each memory label\n",
    "    for i, label in enumerate(memory_labels):\n",
    "        if label < num_classes:\n",
    "            memory_labels_onehot[i, label] = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Label {label} is out of range. Expected between 0 and {num_classes - 1}.\")\n",
    "\n",
    "    # Loop over each memory embedding and compute distances\n",
    "    for i, memory_embedding in enumerate(memory_embeddings):\n",
    "        # One-hot encode the memory label\n",
    "        memory_label_onehot = memory_labels_onehot[i]\n",
    "        \n",
    "        # When two embeddings are identical the cosine = 1 and the Euclidean is 0\n",
    "        # when two embeddings are very different then cosine = -1 and euclidean = inf\n",
    "        # Compute the 1/(1+d) for Euclidean in order to be 1 for identical and 0 for very different,\n",
    "        # Compute the distance (Cosine or Euclidean based on the parameter)\n",
    "        if distance_metric == 'cosine':\n",
    "            dist = cosine_similarity([selected_test_image_embedding], [memory_embedding])[0][0]\n",
    "        elif distance_metric == 'euclidean':\n",
    "            dist = 1/(1+euclidean_distances([selected_test_image_embedding], [memory_embedding])[0][0]) # 1/(1+d)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric. Choose 'cosine' or 'euclidean'.\")\n",
    "        \n",
    "        # print(f'######Iteration {i}#########')\n",
    "        # print(memory_labels[i])\n",
    "        # print(memory_label_onehot)\n",
    "        # print(dist)\n",
    "        # print(f'Before: {distance_sums}')\n",
    "        # Add the distance to the initialized array\n",
    "        distance_sums += memory_label_onehot * dist\n",
    "        # print(f'After: {distance_sums}')\n",
    "        # print('###############')\n",
    "\n",
    "    \n",
    "    # Final calculation based on the distance metric\n",
    "    if predicted_softmax_label is not None:\n",
    "        # print('########## Final #########')\n",
    "        \n",
    "        # Add the predicted value weighted by static distance\n",
    "        distance_sums += predicted_softmax_label * static_distance\n",
    "        \n",
    "        # print(predicted_softmax_label * static_distance)\n",
    "        # print(f'Final: {distance_sums}')\n",
    "\n",
    "    return distance_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_predicted_class` Function\n",
    "\n",
    "The `get_predicted_class` function determines the predicted class based on the cumulative similarity scores (`distance_sums`) computed by the `compute_distance_sums` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_class(distance_sums):\n",
    "    index = np.argmax(distance_sums)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `load_data_cls` Function\n",
    "\n",
    "The `load_data_cls` function loads data from a `.npz` file, which contains test and neighbor class embeddings along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cls(file_path: str):\n",
    "    # Load the .npz file\n",
    "    data = np.load(file_path)\n",
    "\n",
    "    # for key in data.keys():\n",
    "    #     print(f\"{key}: {data[key].shape}\")\n",
    "    \n",
    "    # Extract the arrays\n",
    "    test_cls = data['test_cls']\n",
    "    test_labels = data['test_labels']\n",
    "    neighbor_cls = data['neighbor_cls']\n",
    "    neighbor_labels = data['neighbor_labels']\n",
    "    print(\"Data loaded successfully.\")\n",
    "    \n",
    "    return test_cls, test_labels, neighbor_cls, neighbor_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `load_softmax_predictions` Function\n",
    "\n",
    "The `load_softmax_predictions` function loads softmax predictions from a `.npz` file. These predictions represent the probability distribution over classes for a set of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_softmax_predictions(file_path: str):\n",
    "    # Load the .npz file\n",
    "    data = np.load(file_path)\n",
    "    predicted_softmax_labels= data['predictions']\n",
    "    return predicted_softmax_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_softmax_file_path = f\"output_data/vit_tiny_softmax_predictions_CIFAR10.npz\"\n",
    "predicted_softmax_labels = load_softmax_predictions(predicted_softmax_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing CLS Embeddings with Distance Metrics\n",
    "\n",
    "This script processes class embeddings using a specified distance metric (`cosine` or `euclidean`) and evaluates the performance of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing k=9\n",
      "Data loaded successfully.\n",
      "Test CLS: (10000, 192), Type: <class 'numpy.ndarray'>\n",
      "Test Labels: (10000,), Type: <class 'numpy.ndarray'>\n",
      "Neighbor CLS: (10000, 9, 192), Type: <class 'numpy.ndarray'>\n",
      "Neighbor Labels: (10000, 9), Type: <class 'numpy.ndarray'>\n",
      "Distince Number of Classes: 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.76      1000\n",
      "           1       0.76      0.72      0.74      1000\n",
      "           2       0.84      0.51      0.63      1000\n",
      "           3       0.60      0.54      0.57      1000\n",
      "           4       0.70      0.69      0.69      1000\n",
      "           5       0.63      0.63      0.63      1000\n",
      "           6       0.67      0.87      0.76      1000\n",
      "           7       0.74      0.76      0.75      1000\n",
      "           8       0.77      0.80      0.78      1000\n",
      "           9       0.66      0.81      0.73      1000\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.71      0.71      0.70     10000\n",
      "weighted avg       0.71      0.71      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_list = [9] #[3,5,7,9,11,13,15,17,19]\n",
    "use_softmax_predictions = False\n",
    "\n",
    "for k in k_list:\n",
    "    print(f'Processing k={k}')\n",
    "    file_path = f\"output_data/cls/cls_neighbors_euclidean_{k}.npz\"  # Specify the file path\n",
    "    test_embeddings, test_labels, all_memory_embeddings, all_memory_labels = load_data_cls(file_path)\n",
    "    num_classes = len(np.unique(test_labels, return_counts=False))\n",
    "\n",
    "    print(f\"Test CLS: {test_embeddings.shape}, Type: {type(test_embeddings)}\")\n",
    "    print(f\"Test Labels: {test_labels.shape}, Type: {type(test_labels)}\")\n",
    "    print(f\"Neighbor CLS: {all_memory_embeddings.shape}, Type: {type(all_memory_embeddings)}\")\n",
    "    print(f\"Neighbor Labels: {all_memory_labels.shape}, Type: {type(all_memory_labels)}\")\n",
    "    print(f\"Distince Number of Classes: {num_classes}\")\n",
    "\n",
    "    # For CLS\n",
    "    y_pred = []\n",
    "\n",
    "    distance_metric='cosine'\n",
    "    #distance_metric='euclidean'\n",
    "\n",
    "    for selected_test_image_embedding, memory_embeddings, memory_labels, predicted_softmax_label in zip(test_embeddings, all_memory_embeddings, all_memory_labels, predicted_softmax_labels):\n",
    "        #print(memory_labels)\n",
    "        if use_softmax_predictions:\n",
    "            distance_sums = compute_distance_sums(selected_test_image_embedding, memory_embeddings, memory_labels, predicted_softmax_label= predicted_softmax_label, static_distance = 1, distance_metric=distance_metric, num_classes=num_classes)\n",
    "        else:\n",
    "            distance_sums = compute_distance_sums(selected_test_image_embedding, memory_embeddings, memory_labels, predicted_softmax_label= None, static_distance = 1, distance_metric=distance_metric, num_classes=num_classes)\n",
    "        predicted_class = get_predicted_class(distance_sums)\n",
    "        y_pred.append(predicted_class)\n",
    "        #break\n",
    "\n",
    "    # Compute accuracy\n",
    "    print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `process_lists` Function\n",
    "\n",
    "The `process_lists` function processes a list of lists by either summing or averaging the elements along the columns. It supports both Python lists and NumPy arrays as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lists(lists, mode=\"sum\"):\n",
    "    if not isinstance(lists, list):\n",
    "        raise ValueError(\"Input must be a list of lists.\")\n",
    "    \n",
    "    lists = [lst.tolist() if isinstance(lst, np.ndarray) else lst for lst in lists]\n",
    "    \n",
    "    if not all(isinstance(lst, list) for lst in lists):\n",
    "        raise ValueError(\"Each element in the input must be a list.\")\n",
    "    \n",
    "    if mode not in {\"sum\", \"average\"}:\n",
    "        raise ValueError(\"Mode must be 'sum' or 'average'.\")\n",
    "\n",
    "    # Convert to NumPy array for vectorized operations\n",
    "    array = np.array(lists)\n",
    "    \n",
    "    if mode == \"sum\":\n",
    "        processed = np.sum(array, axis=0)\n",
    "    else:  # mode == \"average\"\n",
    "        processed = np.mean(array, axis=0)\n",
    "    \n",
    "    return processed.tolist()  # Return as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `load_data_patch` Function\n",
    "\n",
    "The `load_data_patch` function loads data from a `.npz` file, which contains test and neighbor class embeddings along with their corresponding labels for patch related info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_patch(file_path: str):\n",
    "    # Load the .npz file\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # Extract the arrays\n",
    "    test_patch = data['test_patch']\n",
    "    test_labels = data['test_labels']\n",
    "    neighbor_patch = data['neighbor_path']\n",
    "    neighbor_labels = data['neighbor_labels']\n",
    "    print(\"Data loaded successfully.\")\n",
    "    \n",
    "    return test_patch, test_labels, neighbor_patch, neighbor_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch-Based Prediction and Evaluation\n",
    "\n",
    "This script processes patch embeddings from test images and memory embeddings to predict classes. It supports two calculation modes (`sum` or `average`) and two distance metrics (`cosine` or `euclidean`). The predictions are evaluated using a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.57      1000\n",
      "           1       0.61      0.56      0.58      1000\n",
      "           2       0.54      0.36      0.43      1000\n",
      "           3       0.51      0.27      0.35      1000\n",
      "           4       0.57      0.45      0.50      1000\n",
      "           5       0.51      0.52      0.52      1000\n",
      "           6       0.55      0.70      0.62      1000\n",
      "           7       0.58      0.69      0.63      1000\n",
      "           8       0.64      0.70      0.67      1000\n",
      "           9       0.54      0.83      0.65      1000\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.56      0.57      0.55     10000\n",
      "weighted avg       0.56      0.57      0.55     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Patch\n",
    "y_pred = []\n",
    "\n",
    "calculation_mode = 'average' # 'average'\n",
    "distance_metric = 'cosine' # 'euclidean'\n",
    "use_softmax_predictions = True\n",
    "\n",
    "for selected_test_image_embedding, specific_memory_embeddings, specific_memory_labels, predicted_softmax_label in zip(test_embeddings, all_memory_embeddings, all_memory_labels, predicted_softmax_labels):\n",
    "    distance_sums_list = []\n",
    "\n",
    "    for patch_embedding, memory_embeddings, memory_labels in zip(selected_test_image_embedding, specific_memory_embeddings, specific_memory_labels):\n",
    "        #print(memory_embeddings.shape)\n",
    "        if use_softmax_predictions:\n",
    "            distance_sums_list.append(compute_distance_sums(patch_embedding, memory_embeddings, memory_labels, predicted_softmax_label= predicted_softmax_label, static_distance = 1, distance_metric=distance_metric))\n",
    "        else:\n",
    "            distance_sums_list.append(compute_distance_sums(patch_embedding, memory_embeddings, memory_labels, predicted_softmax_label= None, static_distance = 1, distance_metric=distance_metric))\n",
    "    distance_sums = process_lists(distance_sums_list, mode=calculation_mode)\n",
    "    predicted_class = get_predicted_class(distance_sums)\n",
    "    y_pred.append(predicted_class)\n",
    "    \n",
    "\n",
    "# Compute accuracy\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
