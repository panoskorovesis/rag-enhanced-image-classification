{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# First make sure to install timm\n!pip install timm\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\nimport timm\nimport torchvision\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:44:18.485958Z","iopub.execute_input":"2025-01-18T12:44:18.488104Z","iopub.status.idle":"2025-01-18T12:44:21.783880Z","shell.execute_reply.started":"2025-01-18T12:44:18.488015Z","shell.execute_reply":"2025-01-18T12:44:21.782723Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Device Selection\n\nWe will opt for gpu, if it's available","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f'Using device: {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:18:32.180725Z","iopub.execute_input":"2025-01-18T12:18:32.180901Z","iopub.status.idle":"2025-01-18T12:18:32.189825Z","shell.execute_reply.started":"2025-01-18T12:18:32.180885Z","shell.execute_reply":"2025-01-18T12:18:32.188931Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Download Dataset\n\nWe can use CIFAR10 or CIFAR100 as our dataset. Since both are really common datasets we will use the `torchvision.datasets` class to load them.\n\nTo select the dataset, modify the `use_CIFAR10` boolean variable accordingly.\n\nWe also apply some basic __preprocessing__:\n\n1. Normalize the dataset by subtracting the _mean_ and dividing with the _std_","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\nuse_CIFAR10 = True\n\n# Get the precomputed mean and std\n# Those are needed to normalize the dataset\n# NOTE: To calculate the mean and std we have to \n# 1. calculate the sum for each channel\n# 2. implement mean and variance formulas\nif use_CIFAR10:\n    mean = (0.4914, 0.4822, 0.4465)\n    std = (0.2023, 0.1994, 0.2010)\nelse:\n    raise NotImplementedError(\"Please compute mean, std for CIFAR100\")\n\n# dataset directory remains the same for both cases\ndataset_directory = \"/kaggle/working/\"\n\n# the transformation also remains the same\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.Normalize(mean, std)\n])\n\n# download the dataset\nif use_CIFAR10:\n    cifar_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=True, download=True, transform=transform)\n    test_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=False, download=True, transform=transform)\nelse:\n    cifar_dataset = torchvision.datasets.CIFAR100(root=dataset_directory, train=True, download=True, transform=transform)\n    test_dataset = torchvision.datasets.CIFAR100(root=dataset_directory, train=False, download=True, transform=transform)\n\nprint(f'Dataset downloaded. Total images: {len(cifar_dataset)}')\n\n# Split the dataset into train / valildation sets\ntrain_size = int(0.9 * len(cifar_dataset))\nval_size = len(cifar_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(cifar_dataset, [train_size, val_size])\n\n# set the batch size to 64\nbatch_size = 64\n\n# Create some dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\nprint(f'Train images: {train_size}')\nprint(f'Validation images: {val_size}')\nprint(f'Test images: {len(test_dataset)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:22:13.424401Z","iopub.execute_input":"2025-01-18T12:22:13.424704Z","iopub.status.idle":"2025-01-18T12:22:14.861301Z","shell.execute_reply.started":"2025-01-18T12:22:13.424685Z","shell.execute_reply":"2025-01-18T12:22:14.860515Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nDataset downloaded. Total images: 50000\nTrain images: 45000\nValidation images: 5000\nTest images: 10000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Useful Methods\n\nBellow we have created some methods to make the code simpler.","metadata":{}},{"cell_type":"markdown","source":"### Get CLS Token\n\nBy taking a look at the [documentation](https://huggingface.co/docs/timm/en/feature_extraction) for the timm library and specifically the __Feature Extraction__ section we can see that in order to get the __last hidden state__ of the model we have to use the `forward_features` method.\n\nSpecifically, this method returns the patch embeddings at the last hidden state, __before pooling is applied__. The return vector is of shape\n\n```\n(batch_size, num_patches + 1, hidden_size)\n```\n\nThe __CLS Token__ is by design the __first of the patch embeddings__\n\nFor example to get the CLS Token of the first image in the batch we would have to do:\n\n```py\nmodel_output[0, 0, :]\n```","metadata":{}},{"cell_type":"code","source":"def get_cls_token(model: timm.models.vision_transformer.VisionTransformer, images: torch.Tensor) -> np.array:\n    # get the last hidden state\n    output = model.forward_features(images)\n\n    # for each image get the cls token\n    # make sure to convert each tensor to numpy\n    cls_tokens = output[:, 0, :].cpu().numpy()\n    \n    return cls_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:47:32.672483Z","iopub.execute_input":"2025-01-18T12:47:32.672782Z","iopub.status.idle":"2025-01-18T12:47:32.676928Z","shell.execute_reply.started":"2025-01-18T12:47:32.672763Z","shell.execute_reply":"2025-01-18T12:47:32.675772Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Extract CLS Tokens for all images in a DataLoader\n\nThis method uses the `get_cls_token` method above to extract all cls tokens from a given dataloader.\n\nFor each image we will also need:\n    \n    1. it's original position (?)\n    2. the label\n\nThe method returns __a dictionary__ with:\n1. __key__: The original position of the image\n2. __value__: A dictionary with `cls_token` and `label` keys","metadata":{}},{"cell_type":"code","source":"def get_dataset_cls_tokens(model: timm.models.vision_transformer.VisionTransformer, loader: torch.utils.data.dataloader.DataLoader\n):\n\n    cls_token_dictionary = {}\n\n    for idx, (images, labels) in tqdm(enumerate(loader), desc=\"Calculating CLS Tokens\", total=len(loader)):\n\n        # this returns a numpy array with shape\n        # (batch_size, hidden_size)\n        cls_tokens = get_cls_token(model=model, images=images)\n\n        # For each image in the batch\n        for idx in range(cls_tokens.shape[0]):\n            cls_token_dictionary[idx] = {\n                \"cls_token\" : cls_tokens[idx, :],\n                \"label\" : labels[idx]\n            }\n\n    return cls_token_dictionary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:47:34.301967Z","iopub.execute_input":"2025-01-18T12:47:34.302234Z","iopub.status.idle":"2025-01-18T12:47:34.306924Z","shell.execute_reply.started":"2025-01-18T12:47:34.302215Z","shell.execute_reply":"2025-01-18T12:47:34.305843Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## Load the model\n\nTo load the model we will use the `timm` library.\n\nWe could also use the `transformers` library and more specific the `ViTForImageClassification`.","metadata":{}},{"cell_type":"code","source":"model = timm.create_model(\n    \"vit_tiny_patch16_224\",  # Pre-trained ViT-Tiny on ImageNet-1k\n    pretrained=True,        # Load pre-trained weights\n    num_classes=10          # Adapt classifier head to CIFAR-10 (10 classes)\n)\n\n# We dont want to train here so we can freeze all the layers\nfor param in model.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T12:47:36.915406Z","iopub.execute_input":"2025-01-18T12:47:36.915718Z","iopub.status.idle":"2025-01-18T12:47:37.110764Z","shell.execute_reply.started":"2025-01-18T12:47:36.915697Z","shell.execute_reply":"2025-01-18T12:47:37.109554Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Generate CLS Tokens for the Train Set","metadata":{}},{"cell_type":"code","source":"train_cls_tokens = get_dataset_cls_tokens(model, train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:10:01.661310Z","iopub.execute_input":"2025-01-18T13:10:01.661621Z","iopub.status.idle":"2025-01-18T13:24:41.981782Z","shell.execute_reply.started":"2025-01-18T13:10:01.661601Z","shell.execute_reply":"2025-01-18T13:24:41.981032Z"}},"outputs":[{"name":"stderr","text":"Calculating CLS Tokens: 100%|██████████| 704/704 [14:40<00:00,  1.25s/it]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## Generate CLS Tokens for the Test Set","metadata":{}},{"cell_type":"code","source":"test_cls_tokens = get_dataset_cls_tokens(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:24:41.982700Z","iopub.execute_input":"2025-01-18T13:24:41.982937Z","iopub.status.idle":"2025-01-18T13:27:57.681655Z","shell.execute_reply.started":"2025-01-18T13:24:41.982919Z","shell.execute_reply":"2025-01-18T13:27:57.680942Z"}},"outputs":[{"name":"stderr","text":"Calculating CLS Tokens: 100%|██████████| 157/157 [03:15<00:00,  1.25s/it]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}