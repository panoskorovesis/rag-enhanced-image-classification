{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myrto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, device, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as t:\n",
    "            for images, labels in t:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(images)\n",
    "                correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        return total_loss / len(train_loader.dataset), correct / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"Evaluate model and return loss, accuracy, and softmax predictions.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss, correct = 0, 0\n",
    "        all_softmax_preds = []\n",
    "\n",
    "        with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as t:\n",
    "            for images, labels in t:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                logits = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "\n",
    "                total_loss += loss.item() * len(images)\n",
    "                correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                # Compute softmax predictions\n",
    "                softmax_preds = torch.nn.functional.softmax(logits, dim=1)\n",
    "                all_softmax_preds.append(softmax_preds.cpu())\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        all_softmax_preds = torch.cat(all_softmax_preds, dim=0)\n",
    "\n",
    "        return avg_loss, accuracy, all_softmax_preds\n",
    "\n",
    "    def train(self, train_loader, test_loader, epochs):\n",
    "        \"\"\"Train model and return softmax predictions on test set.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc, softmax_preds = self.evaluate(test_loader)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Test Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        return softmax_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vit(model_name, num_classes, device, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    Load a ViT model, modify its classification head, and optionally freeze the backbone.\n",
    "    \"\"\"\n",
    "    model = create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    \n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, num_classes, train_loader, test_loader, epochs=5, lr=2e-4, weight_decay=1e-2):\n",
    "    \"\"\"\n",
    "    Fine-tune a ViT model on a dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = load_vit(model_name, num_classes, device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.head.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(model, optimizer, loss_fn, device)\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Training {model_name} for {epochs} epochs on {num_classes}-class dataset\")\n",
    "    start_time = time.time()\n",
    "    softmax_preds = trainer.train(train_loader, test_loader, epochs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Training completed in: {elapsed_time:.2f} seconds\")\n",
    "    return model, softmax_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_softmax_predictions(predictions, filename=\"predictions.npz\"):\n",
    "    \"\"\"\n",
    "    Save softmax predictions to an .npz file.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor or np.ndarray): The softmax predictions.\n",
    "        filename (str): Name of the file to save.\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    np.savez_compressed(filename, predictions=predictions)\n",
    "    print(f\"Predictions saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = \"../../../cifar-10-batches-py-for-pytorch\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset downloaded. Total images: 50000\n",
      "Train images: 45000\n",
      "Validation images: 5000\n",
      "Test images: 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "use_CIFAR10 = True\n",
    "\n",
    "# Get the precomputed mean and std\n",
    "# Those are needed to normalize the dataset\n",
    "# NOTE: To calculate the mean and std we have to \n",
    "# 1. calculate the sum for each channel\n",
    "# 2. implement mean and variance formulas\n",
    "if use_CIFAR10:\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "else:\n",
    "    # raise NotImplementedError(\"Please compute mean, std for CIFAR100\")\n",
    "    mean = (0.5070, 0.4865, 0.4408)\n",
    "    std = (0.2613, 0.2503, 0.2703)\n",
    "\n",
    "# dataset directory remains the same for both cases\n",
    "dataset_directory = \"../../../cifar-10-batches-py-for-pytorch\"\n",
    "\n",
    "\n",
    "# the transformation also remains the same\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# download the dataset\n",
    "if use_CIFAR10:\n",
    "    cifar_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=False, download=True, transform=transform)\n",
    "else:\n",
    "    cifar_dataset = torchvision.datasets.CIFAR100(root=dataset_directory, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root=dataset_directory, train=False, download=True, transform=transform)\n",
    "\n",
    "print(f'Dataset downloaded. Total images: {len(cifar_dataset)}')\n",
    "\n",
    "# Split the dataset into train / valildation sets\n",
    "train_size = int(0.9 * len(cifar_dataset))\n",
    "val_size = len(cifar_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(cifar_dataset, [train_size, val_size])\n",
    "\n",
    "# set the batch size to 64\n",
    "batch_size = 64\n",
    "\n",
    "# Create some dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Train images: {train_size}')\n",
    "print(f'Validation images: {val_size}')\n",
    "print(f'Test images: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vit_tiny_patch16_224 for 5 epochs on 10-class dataset\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:01<00:00,  5.82batch/s, loss=1.27] \n",
      "Testing: 100%|██████████| 157/157 [00:29<00:00,  5.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1576, Accuracy: 0.6170\n",
      "Test Loss: 0.8435, Accuracy: 0.7193\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:16<00:00,  5.16batch/s, loss=0.596]\n",
      "Testing: 100%|██████████| 157/157 [00:30<00:00,  5.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7927, Accuracy: 0.7300\n",
      "Test Loss: 0.7535, Accuracy: 0.7432\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:06<00:00,  5.58batch/s, loss=0.615]\n",
      "Testing: 100%|██████████| 157/157 [00:26<00:00,  5.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7365, Accuracy: 0.7461\n",
      "Test Loss: 0.7224, Accuracy: 0.7522\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:16<00:00,  5.16batch/s, loss=0.39] \n",
      "Testing: 100%|██████████| 157/157 [00:26<00:00,  6.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7099, Accuracy: 0.7544\n",
      "Test Loss: 0.7041, Accuracy: 0.7607\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:11<00:00,  5.35batch/s, loss=0.748]\n",
      "Testing: 100%|██████████| 157/157 [00:32<00:00,  4.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6940, Accuracy: 0.7595\n",
      "Test Loss: 0.6937, Accuracy: 0.7605\n",
      "Training completed in: 795.96 seconds\n",
      "Predictions saved to vit_tiny_predictions.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune ViT-Tiny on CIFAR-10  test implementation\n",
    "num_classes = 10  # CIFAR-10\n",
    "vit_tiny_model, vit_tiny_softmax_preds = train_model(\"vit_tiny_patch16_224\", num_classes, train_loader, test_loader)\n",
    "save_softmax_predictions(vit_tiny_softmax_preds, \"vit_tiny_predictions.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
