{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myrto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class \n",
    "\n",
    "The `Trainer` class manages the training and evaluation process. It encapsulates the following functionalities:\n",
    "\n",
    "1. **Initialization**: \n",
    "   - Accepts the model, optimizer, loss function, and device (CPU/GPU), ensuring the model is moved to the correct device.\n",
    "   - Optionally supports a learning rate scheduler.\n",
    "\n",
    "2. **Training**: The `train_epoch` method:\n",
    "   - Iterates through training batches using a `tqdm` progress bar.\n",
    "   - Performs forward passes, computes loss, backpropagates, and updates model parameters.\n",
    "   - Returns the average training loss and accuracy for the epoch.\n",
    "\n",
    "3. **Evaluation**: The `evaluate` method:\n",
    "   - Runs the model on a validation or test set in evaluation mode (`model.eval()`).\n",
    "   - Computes accuracy and average loss, displaying progress with a `tqdm` bar.\n",
    "   - Computes and stores softmax probabilities for each sample in the test set.\n",
    "\n",
    "4. **Full Training Loop**: The `train` method:\n",
    "   - Combines `train_epoch` and `evaluate` for multiple epochs.\n",
    "   - Logs training loss, validation loss, and accuracy after each epoch.\n",
    "   - Returns the softmax predictions for the test set after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, device, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        with tqdm(train_loader, desc=\"Training\", unit=\"batch\") as t:\n",
    "            for images, labels in t:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(images)\n",
    "                correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        return total_loss / len(train_loader.dataset), correct / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"Evaluate model and return loss, accuracy, and softmax predictions.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss, correct = 0, 0\n",
    "        all_softmax_preds = []\n",
    "\n",
    "        with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as t:\n",
    "            for images, labels in t:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                logits = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "\n",
    "                total_loss += loss.item() * len(images)\n",
    "                correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                # Compute softmax predictions\n",
    "                softmax_preds = torch.nn.functional.softmax(logits, dim=1)\n",
    "                all_softmax_preds.append(softmax_preds.cpu())\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        all_softmax_preds = torch.cat(all_softmax_preds, dim=0)\n",
    "\n",
    "        return avg_loss, accuracy, all_softmax_preds\n",
    "\n",
    "    def train(self, train_loader, test_loader, epochs):\n",
    "        \"\"\"Train model and return softmax predictions and final test accuracy.\"\"\"\n",
    "        final_test_acc = 0  \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc, softmax_preds = self.evaluate(test_loader)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Test Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            final_test_acc = val_acc \n",
    "        return softmax_preds, final_test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vision Transformer (ViT) Model\n",
    "\n",
    "The `load_vit` function is responsible for loading a Vision Transformer (ViT) model, modifying its classification head, and optionally freezing the backbone layers.\n",
    "\n",
    "1. **Model Creation**:\n",
    "   - Loads a pretrained ViT model using the specified `model_name`.\n",
    "   - Adjusts the model's classification head to match the required number of classes (`num_classes`).\n",
    "\n",
    "2. **Freezing the Backbone**:\n",
    "   - If `freeze_backbone=True`, all model parameters except for the classification head are frozen.\n",
    "\n",
    "3. **Device Assignment**:\n",
    "   - Moves the model to the specified `device` (CPU/GPU) for optimized computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vit(model_name, num_classes, device, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    Load a ViT model, modify its classification head, and optionally freeze the backbone.\n",
    "    \"\"\"\n",
    "    model = create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "    \n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.head.parameters():\n",
    "            param.requires_grad = True  # Only train the classification head\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Model Training Function\n",
    "\n",
    "The `train_model` function is responsible for fine-tuning a Vision Transformer (ViT) model on a given dataset. It provides the following functionalities:\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   - Loads a ViT model using `load_vit`, ensuring it is correctly configured with the specified `model_name`, `num_classes`, and `device`.\n",
    "\n",
    "2. **Optimizer and Loss Function**:\n",
    "   - Uses AdamW as the optimizer.\n",
    "   - Defines cross-entropy loss for multi-class classification tasks.\n",
    "\n",
    "3. **Training Process**:\n",
    "   - Initializes a `Trainer` instance to manage training and evaluation.\n",
    "   - Iterates through multiple epochs, logging performance metrics after each epoch.\n",
    "   - Computes and returns softmax predictions from the trained model.\n",
    "\n",
    "4. **Performance Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, num_classes, train_loader, test_loader, epochs=10, lr=1e-3, weight_decay=1e-2):\n",
    "    \"\"\"\n",
    "    Fine-tune a ViT model on a dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = load_vit(model_name, num_classes, device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.head.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(model, optimizer, loss_fn, device)\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Training {model_name} for {epochs} epochs on {num_classes}-class dataset\")\n",
    "    start_time = time.time()\n",
    "    softmax_preds = trainer.train(train_loader, test_loader, epochs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Training completed in: {elapsed_time:.2f} seconds\")\n",
    "    return model, softmax_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Predictions Saving Function\n",
    "\n",
    "The `save_softmax_predictions` function is used to save the softmax predictions generated by a trained model into a compressed `.npz` file for later analysis. It provides the following functionalities:\n",
    "\n",
    "1. **Data Handling**:\n",
    "   - Accepts softmax predictions as a `torch.Tensor` or `numpy.ndarray`.\n",
    "   - Converts `torch.Tensor` predictions to a NumPy array if necessary.\n",
    "\n",
    "2. **File Saving**:\n",
    "   - Saves the predictions in a compressed `.npz` format using `numpy.savez_compressed`.\n",
    "   - Allows for specifying a custom filename.\n",
    "\n",
    "3. **Logging**:\n",
    "   - Prints a confirmation message indicating where the predictions were saved.\n",
    "\n",
    "This function ensures that model outputs can be efficiently stored and retrieved for further evaluation, such as model comparison or uncertainty estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_softmax_predictions(predictions, filename=\"predictions.npz\"):\n",
    "    \"\"\"\n",
    "    Save softmax predictions to an .npz file.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor or np.ndarray): The softmax predictions.\n",
    "        filename (str): Name of the file to save.\n",
    "    \"\"\"\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    np.savez_compressed(filename, predictions=predictions)\n",
    "    print(f\"Predictions saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Loading Function\n",
    "\n",
    "The dataset loading process is managed through pickled `DataLoader` objects.\n",
    "   - The `load_dataloader` function reads and unpickles `DataLoader` objects stored in `.pkl` files.\n",
    "   - `train_loader`, `val_loader`, and `test_loader` are loaded from specified paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 45000\n",
      "Validation images: 5000\n",
      "Test images: 10000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Paths to the pickle files\n",
    "train_loader_path = r\"D:\\Master\\ComputerVision\\rag-enhanced-image-classification\\src\\development\\train_loader.pkl\"\n",
    "val_loader_path = r\"D:\\Master\\ComputerVision\\rag-enhanced-image-classification\\src\\development\\val_loader.pkl\"\n",
    "test_loader_path = r\"D:\\Master\\ComputerVision\\rag-enhanced-image-classification\\src\\development\\test_loader.pkl\"\n",
    "\n",
    "# Function to load pickled DataLoader objects\n",
    "def load_dataloader(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load the DataLoaders\n",
    "train_loader = load_dataloader(train_loader_path)\n",
    "val_loader = load_dataloader(val_loader_path)\n",
    "test_loader = load_dataloader(test_loader_path)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f'Train images: {len(train_loader.dataset)}')\n",
    "print(f'Validation images: {len(val_loader.dataset)}')\n",
    "print(f'Test images: {len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune ViT-Tiny Classifier head on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vit_tiny_patch16_224 for 10 epochs on 10-class dataset\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:15<00:00,  5.19batch/s, loss=0.555]\n",
      "Testing: 100%|██████████| 157/157 [00:33<00:00,  4.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1197, Accuracy: 0.6310\n",
      "Test Loss: 0.8388, Accuracy: 0.7194\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [01:58<00:00,  5.92batch/s, loss=1.31] \n",
      "Testing: 100%|██████████| 157/157 [00:32<00:00,  4.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7890, Accuracy: 0.7313\n",
      "Test Loss: 0.7555, Accuracy: 0.7437\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:31<00:00,  4.64batch/s, loss=1.73] \n",
      "Testing: 100%|██████████| 157/157 [00:28<00:00,  5.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7350, Accuracy: 0.7493\n",
      "Test Loss: 0.7205, Accuracy: 0.7560\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [01:55<00:00,  6.11batch/s, loss=0.62] \n",
      "Testing: 100%|██████████| 157/157 [00:24<00:00,  6.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7098, Accuracy: 0.7550\n",
      "Test Loss: 0.7026, Accuracy: 0.7563\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:03<00:00,  5.69batch/s, loss=0.16] \n",
      "Testing: 100%|██████████| 157/157 [00:30<00:00,  5.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6936, Accuracy: 0.7606\n",
      "Test Loss: 0.6929, Accuracy: 0.7618\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:02<00:00,  5.73batch/s, loss=0.777]\n",
      "Testing: 100%|██████████| 157/157 [00:27<00:00,  5.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6831, Accuracy: 0.7638\n",
      "Test Loss: 0.6866, Accuracy: 0.7629\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:36<00:00,  4.51batch/s, loss=0.435]\n",
      "Testing: 100%|██████████| 157/157 [00:33<00:00,  4.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6750, Accuracy: 0.7658\n",
      "Test Loss: 0.6815, Accuracy: 0.7650\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [01:56<00:00,  6.04batch/s, loss=0.731]\n",
      "Testing: 100%|██████████| 157/157 [00:26<00:00,  5.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6689, Accuracy: 0.7689\n",
      "Test Loss: 0.6822, Accuracy: 0.7675\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:09<00:00,  5.44batch/s, loss=0.504]\n",
      "Testing: 100%|██████████| 157/157 [00:27<00:00,  5.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6646, Accuracy: 0.7705\n",
      "Test Loss: 0.6741, Accuracy: 0.7690\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [02:08<00:00,  5.48batch/s, loss=0.725]\n",
      "Testing: 100%|██████████| 157/157 [00:26<00:00,  5.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6603, Accuracy: 0.7721\n",
      "Test Loss: 0.6772, Accuracy: 0.7657\n",
      "Training completed in: 1590.16 seconds\n",
      "Predictions saved to vit_tiny_softmax_predictions_CIFAR10.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune ViT-Tiny on CIFAR-10  test implementation\n",
    "num_classes = 10  # CIFAR-10\n",
    "vit_tiny_model, vit_tiny_softmax_preds = train_model(\"vit_tiny_patch16_224\", num_classes, train_loader, test_loader)\n",
    "save_softmax_predictions(vit_tiny_softmax_preds, \"vit_tiny_softmax_predictions_CIFAR10.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Predictions Loading and Inspection\n",
    "\n",
    "The softmax predictions saved in an `.npz` file can be loaded and inspected using the following process:\n",
    "   - Defines the path to the `.npz` file containing softmax predictions.\n",
    "   - Uses `numpy.load()` to open the compressed file.\n",
    "   - Verifies that the key \"predictions\" exists and prints the shape of the stored softmax predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['predictions']\n",
      "Shape of softmax predictions: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = r\"D:\\Master\\ComputerVision\\rag-enhanced-image-classification\\src\\development\\vit_tiny_softmax_predictions_CIFAR10.npz\"\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "# Check the keys in the file\n",
    "print(\"Keys in the file:\", data.files)\n",
    "\n",
    "# Assuming the softmax predictions are stored under the key \"predictions\"\n",
    "if \"predictions\" in data:\n",
    "    print(\"Shape of softmax predictions:\", data[\"predictions\"].shape)\n",
    "else:\n",
    "    print(\"Key 'predictions' not found. Available keys:\", data.files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
